{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5182fd3b-fa90-4a6d-b146-4a5c631368eb",
   "metadata": {},
   "source": [
    "# Formalia\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/TheYuanLiao/comsocsci2025/wiki/Assignments) carefully before proceeding. The page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment.\n",
    "\n",
    "These exercises are a subset of the exercises you did in class and you could just copy-paste the solution you developed in class.\n",
    "\n",
    "__If you fail to follow these simple instructions, it will negatively impact your grade!__\n",
    "\n",
    "**Due date and time**: The assignment is due on Mar 4th at 23:59. Hand in your Jupyter notebook file (with extension `.ipynb`) via DTU Learn _(Assignment 1)_. \n",
    "\n",
    "Remember to include in the first cell of your notebook:\n",
    "* the link to your group's Git repository \n",
    "* group members' contributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323be069",
   "metadata": {},
   "source": [
    "Link to Git repository: https://github.com/cruesli/CSS_group13\n",
    "\n",
    "# Contributions\n",
    "\n",
    "Simon 33%$\\\\$\n",
    "Gustav 33%$\\\\$\n",
    "Magnus 33%$\\\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182d781-3727-4ff5-9b58-689381202a99",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping\n",
    "Week 1, ex 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e8e50-307c-4315-bed5-1de2fd37d743",
   "metadata": {},
   "source": [
    "> **Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science**    \n",
    ">\n",
    "> You can find the programme of the 2023 edition of the conference at [this link](https://ic2s2-2023.org/program). As you can see the conference programme included many different contributions: keynote presentations, parallel talks, tutorials, posters. \n",
    "> 1. Inspect the HTML of the page and use web-scraping to get the names of all researchers that contributed to the conference in 2023. The goal is the following: (i) get as many names as possible including: keynote speakers, chairs, authors of parallel talks and authors of posters; (ii) ensure that the collected names are complete and accuarate as reported in the website (e.g. both first name and family name); (iii) ensure that no name is repeated multiple times with slightly different spelling. \n",
    "> 2. Some instructions for success: \n",
    ">    * First, inspect the page through your web browser to identify the elements of the page that you want to collect. Ensure you understand the hierarchical structure of the page, and where the elements you are interested in are located within this nested structure.   \n",
    ">    * Use the [BeautifulSoup Python package](https://pypi.org/project/beautifulsoup4/) to navigate through the hierarchy and extract the elements you need from the page. \n",
    ">    * You can use the [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method to find elements that match specific filters. Check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) of the library for detailed explanations on how to set filters.  \n",
    ">    * Parse the strings to ensure that you retrieve \"clean\" author names (e.g. remove commas, or other unwanted charachters)\n",
    ">    * The overall idea is to adapt the procedure I have used [here](https://nbviewer.org/github/lalessan/comsocsci2023/blob/master/additional_notebooks/ScreenScraping.ipynb) for the specific page you are scraping. \n",
    "> 3. Create the set of unique researchers that joined the conference and *store it into a file*.\n",
    ">     * *Important:* If you notice any issue with the list of names you have collected (e.g. duplicate/incorrect names), come up with a strategy to clean your list as much as possible. \n",
    "> 4. *Optional:* For a more complete represenation of the field, include in your list: (i) the names of researchers from the programme committee of the conference, that can be found at [this link](https://ic2s2-2023.org/program_committee); (ii) the organizers of tutorials, that can be found at [this link](https://ic2s2-2023.org/tutorials)\n",
    "> 5. How many unique researchers do you get?\n",
    "> 6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices __(answer in max 150 words)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a700502",
   "metadata": {},
   "source": [
    "We started by by inspecting the HTML of the page and identified that the names of the speakers were contained within unordered lists (ul) with class name \"nav_list\", and the names of the chair's was contained within a h2 header. Using BeatifulSoups find_all() function we retrieved all ul's with class name \"nav_list\". Within each of the ul's the names of the authors are contained within a <i> tag (italic), which were used to retrieve the names. The process was then to loop through all the ul lists, find all <i> within each list. Then loop through all <i<>s, split up the names and remove unwanted characters to get clean names. The same was done with the h2 headers were the word chair was removed to leave us with the names. Lastly we removed all duplicates. This approach should retrieve all the names from the program page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9ef9e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique names: 1523\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "Link = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(Link)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "ulist = soup.find_all('ul',{'class':'nav_list'})\n",
    "h2s = soup.find_all('h2')\n",
    "names = []\n",
    "for h2 in h2s:\n",
    "    nsl = h2.find_all('i')\n",
    "    if nsl:\n",
    "        ns = str(nsl[0])[10:]\n",
    "        cleaned_ns = re.sub(r\"[^\\w\\s\\,]\", \"\", ns) # Removes any unwanted characters\n",
    "        cleaned_ns = cleaned_ns[:len(cleaned_ns)-1] # Removes the i left over from </i>\n",
    "        names.append(cleaned_ns)\n",
    "for lists in ulist:\n",
    "    nsl = lists.find_all('i')\n",
    "    for nl in nsl:\n",
    "        ns = nl.text.split(', ')\n",
    "        cleaned_ns = [re.sub(r\"[^\\w\\s\\,]\", \"\", n) for n in ns] # Removes any unwanted characters\n",
    "        names.append(cleaned_ns)\n",
    "names = [item for sublist in names for item in sublist] # flatten list\n",
    "unique_names = sorted(list(set(names)), key = lambda name: name[0]) # Remove any duplicates and sort in alphabetical order\n",
    "print(f'Number of unique names: {len(unique_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17914c-f125-4f70-b1a5-0c56641a0ea0",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "Week 2, ex 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059f1b-25c3-47eb-8278-903bd7608576",
   "metadata": {},
   "source": [
    "> **Exercise: Ready made data vs Custom made data** In this exercise, I want to make sure you have understood they key points of my lecture and the reading. \n",
    ">\n",
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__.\n",
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059f1b-25c3-47eb-8278-903bd7608576",
   "metadata": {},
   "source": [
    "> 1. Centola's experiment used custom made data, meaning the environment and interactions were fully designed by the researchers. One big pro is that they could presicely trach how people's behaviour spread and after identify which factors caused these shifts. By designing the setup themselves, they can minimize unpredictable outside factors that might otherwise blur the results. The downside is that this highly controlled setting may not fully represent real world situations very well, so the findings may be less generalizable. On the other hand, Nicolaide's study used data collected from real people's everyday interactions. This makes the results more realistic, since it shows how people behave without being prompted by an experiment. However, ready-made data often includes many uncontrollable variables, making it harder to specify what led to certain outcomes. Researchers also risk missing important details if the data was not gathered with their specific research questions in mind. \n",
    "> 2. Centola's controlled envrionment lets us see precisely how each factor affects behaviour, making it easier to discuss cause and effect with confidence. But because people most people don't live under the tight conditions set by the experiment, we have to be careful about generalizing the results. Nicolaides' real world data gives a more realistic picture of how behaviours spread in everyday life, even though its harder to figure out which factors matter most, because theres more noise.  Therefore, we interpret Centola's findings with stronger centainty about specific influences, while Nicolaide's study shows how these processes play out in a natural setting. Both view can inform each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c7606-9542-4ecd-9f6e-1c5e298883d6",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API\n",
    "Week 3, ex 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed489a-1f99-4124-af1b-97f332fb0fe1",
   "metadata": {},
   "source": [
    "> **Exercise : Collecting Research Articles from IC2S2 Authors**\n",
    ">\n",
    ">In this exercise, we'll leverage the OpenAlex API to gather information on research articles authored by participants of the IC2S2 2024 (NOT 2023) conference, referred to as *IC2S2 authors*. **Before you start, please ensure you read through the entire exercise.**\n",
    ">\n",
    "> \n",
    "> **Steps:**\n",
    ">  \n",
    "> 1. **Retrieve Data:** Starting with the *authors* you identified in Week 2, Exercise 2, use the OpenAlex API [works endpoint](https://docs.openalex.org/api-entities/works) to fetch the research articles they have authored. For each article, retrieve the following details:\n",
    ">    - _id_: The unique OpenAlex ID for the work.\n",
    ">    - _publication_year_: The year the work was published.\n",
    ">    - _cited_by_count_: The number of times the work has been cited by other works.\n",
    ">    - _author_ids_: The OpenAlex IDs for the authors of the work.\n",
    ">    - _title_: The title of the work.\n",
    ">    - _abstract_inverted_index_: The abstract of the work, formatted as an inverted index.\n",
    "> \n",
    ">     **Important Note on Paging:** By default, the OpenAlex API limits responses to 25 works per request. For more efficient data retrieval, I suggest to adjust this limit to 200 works per request. Even with this adjustment, you will need to implement pagination to access all available works for a given query. This ensures you can systematically retrieve the complete set of works beyond the initial 200. Find guidance on implementing pagination [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging).\n",
    ">\n",
    "> 2. **Data Storage:** Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n",
    ">    - The *IC2S2 papers* dataset should include: *id, publication\\_year, cited\\_by\\_count, author\\_ids*.\n",
    ">    - The *IC2S2 abstracts* dataset should include: *id, title, abstract\\_inverted\\_index*.\n",
    ">  \n",
    ">\n",
    "> **Filters:**\n",
    "> To ensure the data we collect is relevant and manageable, apply the following filters:\n",
    "> \n",
    ">    - Only include *IC2S2 authors* with a total work count between 5 and 5,000.\n",
    ">    - Retrieve only works that have received more than 10 citations.\n",
    ">    - Limit to works authored by fewer than 10 individuals.\n",
    ">    - Include only works relevant to Computational Social Science (focusing on: Sociology OR Psychology OR Economics OR Political Science) AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science), as defined by their [Concepts](https://docs.openalex.org/api-entities/works/work-object#concepts). *Note*: here we only consider Concepts at *level=0* (the most coarse definition of concepts). \n",
    ">\n",
    "> **Efficiency Tips:**\n",
    "> Writing efficient code in this exercise is **crucial**. To speed up your process:\n",
    "> - **Apply filters directly in your request:** When possible, use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) of the *works* endpoint to apply the filters above directly in your API request, ensuring only relevant data is returned. Learn about combining multiple filters [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists).  \n",
    "> - **Bulk requests:** Instead of sending one request for each author, you can use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) to query works by multiple authors in a single request. *Note: My testing suggests that can only include up to 25 authors per request.*\n",
    "> - **Use multiprocessing:** Implement multiprocessing to handle multiple requests simultaneously. I highly recommmend [Joblib’s Parallel](https://joblib.readthedocs.io/en/stable/) function for that, and [tqdm](https://tqdm.github.io/) can help monitor progress of your jobs. Remember to stay within [the rate limit](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) of 10 requests per second.\n",
    ">\n",
    ">\n",
    ">   \n",
    "> For reference, employing these strategies allowed me to fetch the data in about 30 seconds using 5 cores on my laptop. I obtained a dataset of approximately 25 MB (including both the *IC2S2 abstracts* and *IC2S2 papers* files).\n",
    "> \n",
    ">\n",
    "> **Data Overview and Reflection questions:** Answer the following questions: \n",
    "> - **Dataset summary.** How many works are listed in your *IC2S2 papers* dataframe? How many unique researchers have co-authored these works? \n",
    "> - **Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? __(answer in max 150 words)__\n",
    "> - **Filtering Criteria and Dataset Relevance** Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? __(answer in max 150 words)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3412c8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found id\n",
      "1/1523\n",
      "No id found\n",
      "2/1523\n",
      "No id found\n",
      "3/1523\n",
      "Found id\n",
      "4/1523\n",
      "Found id\n",
      "5/1523\n",
      "Found id\n",
      "6/1523\n",
      "Found id\n",
      "7/1523\n",
      "No id found\n",
      "8/1523\n",
      "Found id\n",
      "9/1523\n",
      "Found id\n",
      "10/1523\n",
      "Found id\n",
      "11/1523\n",
      "Found id\n",
      "12/1523\n",
      "Found id\n",
      "13/1523\n",
      "Found id\n",
      "14/1523\n",
      "Found id\n",
      "15/1523\n",
      "Found id\n",
      "16/1523\n",
      "Found id\n",
      "17/1523\n",
      "Found id\n",
      "18/1523\n",
      "Found id\n",
      "19/1523\n",
      "Found id\n",
      "20/1523\n",
      "Found id\n",
      "21/1523\n",
      "No id found\n",
      "22/1523\n",
      "Found id\n",
      "23/1523\n",
      "Found id\n",
      "24/1523\n",
      "Found id\n",
      "25/1523\n",
      "Found id\n",
      "26/1523\n",
      "Found id\n",
      "27/1523\n",
      "Found id\n",
      "28/1523\n",
      "Found id\n",
      "29/1523\n",
      "No id found\n",
      "30/1523\n",
      "Found id\n",
      "31/1523\n",
      "Found id\n",
      "32/1523\n",
      "Found id\n",
      "33/1523\n",
      "Found id\n",
      "34/1523\n",
      "Found id\n",
      "35/1523\n",
      "Found id\n",
      "36/1523\n",
      "Found id\n",
      "37/1523\n",
      "Found id\n",
      "38/1523\n",
      "Found id\n",
      "39/1523\n",
      "Found id\n",
      "40/1523\n",
      "Found id\n",
      "41/1523\n",
      "Found id\n",
      "42/1523\n",
      "Found id\n",
      "43/1523\n",
      "Found id\n",
      "44/1523\n",
      "Found id\n",
      "45/1523\n",
      "Found id\n",
      "46/1523\n",
      "Found id\n",
      "47/1523\n",
      "Found id\n",
      "48/1523\n",
      "Found id\n",
      "49/1523\n",
      "Found id\n",
      "50/1523\n",
      "Found id\n",
      "51/1523\n",
      "Found id\n",
      "52/1523\n",
      "Found id\n",
      "53/1523\n",
      "Found id\n",
      "54/1523\n",
      "Found id\n",
      "55/1523\n",
      "Found id\n",
      "56/1523\n",
      "No id found\n",
      "57/1523\n",
      "Found id\n",
      "58/1523\n",
      "Found id\n",
      "59/1523\n",
      "Found id\n",
      "60/1523\n",
      "Found id\n",
      "61/1523\n",
      "Found id\n",
      "62/1523\n",
      "Found id\n",
      "63/1523\n",
      "Found id\n",
      "64/1523\n",
      "Found id\n",
      "65/1523\n",
      "Found id\n",
      "66/1523\n",
      "Found id\n",
      "67/1523\n",
      "Found id\n",
      "68/1523\n",
      "Found id\n",
      "69/1523\n",
      "Found id\n",
      "70/1523\n",
      "Found id\n",
      "71/1523\n",
      "No id found\n",
      "72/1523\n",
      "Found id\n",
      "73/1523\n",
      "Found id\n",
      "74/1523\n",
      "Found id\n",
      "75/1523\n",
      "Found id\n",
      "76/1523\n",
      "Found id\n",
      "77/1523\n",
      "Found id\n",
      "78/1523\n",
      "Found id\n",
      "79/1523\n",
      "Found id\n",
      "80/1523\n",
      "Found id\n",
      "81/1523\n",
      "Found id\n",
      "82/1523\n",
      "Found id\n",
      "83/1523\n",
      "Found id\n",
      "84/1523\n",
      "Found id\n",
      "85/1523\n",
      "Found id\n",
      "86/1523\n",
      "Found id\n",
      "87/1523\n",
      "Found id\n",
      "88/1523\n",
      "Found id\n",
      "89/1523\n",
      "Found id\n",
      "90/1523\n",
      "No id found\n",
      "91/1523\n",
      "Found id\n",
      "92/1523\n",
      "Found id\n",
      "93/1523\n",
      "Found id\n",
      "94/1523\n",
      "No id found\n",
      "95/1523\n",
      "Found id\n",
      "96/1523\n",
      "Found id\n",
      "97/1523\n",
      "Found id\n",
      "98/1523\n",
      "Found id\n",
      "99/1523\n",
      "Found id\n",
      "100/1523\n",
      "Found id\n",
      "101/1523\n",
      "Found id\n",
      "102/1523\n",
      "Found id\n",
      "103/1523\n",
      "Found id\n",
      "104/1523\n",
      "Found id\n",
      "105/1523\n",
      "Found id\n",
      "106/1523\n",
      "Found id\n",
      "107/1523\n",
      "Found id\n",
      "108/1523\n",
      "Found id\n",
      "109/1523\n",
      "Found id\n",
      "110/1523\n",
      "Found id\n",
      "111/1523\n",
      "Found id\n",
      "112/1523\n",
      "Found id\n",
      "113/1523\n",
      "Found id\n",
      "114/1523\n",
      "Found id\n",
      "115/1523\n",
      "Found id\n",
      "116/1523\n",
      "Found id\n",
      "117/1523\n",
      "Found id\n",
      "118/1523\n",
      "Found id\n",
      "119/1523\n",
      "Found id\n",
      "120/1523\n",
      "Found id\n",
      "121/1523\n",
      "Found id\n",
      "122/1523\n",
      "Found id\n",
      "123/1523\n",
      "Found id\n",
      "124/1523\n",
      "Found id\n",
      "125/1523\n",
      "Found id\n",
      "126/1523\n",
      "Found id\n",
      "127/1523\n",
      "Found id\n",
      "128/1523\n",
      "Found id\n",
      "129/1523\n",
      "Found id\n",
      "130/1523\n",
      "Found id\n",
      "131/1523\n",
      "Found id\n",
      "132/1523\n",
      "Found id\n",
      "133/1523\n",
      "Found id\n",
      "134/1523\n",
      "Found id\n",
      "135/1523\n",
      "Found id\n",
      "136/1523\n",
      "Found id\n",
      "137/1523\n",
      "Found id\n",
      "138/1523\n",
      "No id found\n",
      "139/1523\n",
      "Found id\n",
      "140/1523\n",
      "Found id\n",
      "141/1523\n",
      "Found id\n",
      "142/1523\n",
      "Found id\n",
      "143/1523\n",
      "Found id\n",
      "144/1523\n",
      "Found id\n",
      "145/1523\n",
      "Found id\n",
      "146/1523\n",
      "Found id\n",
      "147/1523\n",
      "Found id\n",
      "148/1523\n",
      "Found id\n",
      "149/1523\n",
      "Found id\n",
      "150/1523\n",
      "Found id\n",
      "151/1523\n",
      "Found id\n",
      "152/1523\n",
      "Found id\n",
      "153/1523\n",
      "Found id\n",
      "154/1523\n",
      "No id found\n",
      "155/1523\n",
      "Found id\n",
      "156/1523\n",
      "Found id\n",
      "157/1523\n",
      "Found id\n",
      "158/1523\n",
      "No id found\n",
      "159/1523\n",
      "Found id\n",
      "160/1523\n",
      "No id found\n",
      "161/1523\n",
      "Found id\n",
      "162/1523\n",
      "No id found\n",
      "163/1523\n",
      "Found id\n",
      "164/1523\n",
      "Found id\n",
      "165/1523\n",
      "Found id\n",
      "166/1523\n",
      "Found id\n",
      "167/1523\n",
      "Found id\n",
      "168/1523\n",
      "Found id\n",
      "169/1523\n",
      "Found id\n",
      "170/1523\n",
      "Found id\n",
      "171/1523\n",
      "No id found\n",
      "172/1523\n",
      "Found id\n",
      "173/1523\n",
      "Found id\n",
      "174/1523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m params \u001b[38;5;241m=\u001b[39m { \u001b[38;5;66;03m# Query parameters. Added filter to only include authors who have published between 5 and 5000 works\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay_name.search:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,works_count:>5|<5000\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Send request to OpenAlex API\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    618\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Retrieving Author Ids from web scraped names\n",
    "# This can probably be massively improved upon\n",
    "import pandas as pd\n",
    "BASE_URL = 'https://api.openalex.org/'\n",
    "RESOURCE = 'works'\n",
    "complete_url = BASE_URL + RESOURCE\n",
    "\n",
    "base_url = \"https://api.openalex.org/authors\"\n",
    "\n",
    "author_ids = {}\n",
    "n = len(unique_names)\n",
    "i = 0\n",
    "for name in unique_names:\n",
    "    \n",
    "    params = { # Query parameters. Added filter to only include authors who have published between 5 and 5000 works\n",
    "        'filter': f'display_name.search:{name},works_count:>5|<5000',\n",
    "        'select': 'id'\n",
    "    }\n",
    "    \n",
    "    # Send request to OpenAlex API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['results']:\n",
    "            author_ids[name] = data.get('results')[0]['id']\n",
    "            print('Found id')\n",
    "        else:\n",
    "            print('No id found')\n",
    "    else:\n",
    "        print('Failed to retrieve')\n",
    "    i += 1\n",
    "    print(f'{i}/{n}')\n",
    "\n",
    "print(len(author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "440db472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 2791.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Datasets saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "def fetch_papers_for_authors(authors_batch, batch_index, total_batches):\n",
    "    papers_dataset = []\n",
    "    abstract_dataset = []\n",
    "\n",
    "    author_filters = '|'.join([f\"\\\"{author_id}\\\"\" for author_id in authors_batch])\n",
    "    \n",
    "    page = 1\n",
    "    while True:\n",
    "        params = {\n",
    "            \"filter\": f\"authorships.author.id:\\\"{author_filters}\\\",cited_by_count:>10,authors_count:<10\",\n",
    "            \"per_page\": 200,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(URL, params=params, timeout=10).json()\n",
    "                break \n",
    "            except requests.exceptions.RequestException:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(5) \n",
    "                else:\n",
    "                    return [], [], f\"Skipping batch {batch_index} due to repeated request failures.\"\n",
    "\n",
    "    \n",
    "        if 'results' not in response or not response['results']:\n",
    "            break\n",
    "\n",
    "        for paper in response['results']:\n",
    "            first_concept, second_concept = False, False\n",
    "\n",
    "            for concept in paper.get('concepts', []):\n",
    "                if concept.get('level') != 0:\n",
    "                    continue\n",
    "                if concept.get('display_name') in ['Sociology', 'Psychology', 'Economics', 'Political Science']:\n",
    "                    first_concept = True\n",
    "                if concept.get('display_name') in ['Mathematics', 'Computer Science', 'Physics']:\n",
    "                    second_concept = True\n",
    "\n",
    "            if not (first_concept and second_concept):\n",
    "                continue\n",
    "\n",
    "            paper_info = {\n",
    "                'id': paper.get('id'),\n",
    "                'publication_year': paper.get('publication_year'),\n",
    "                'cited_by_count': paper.get('cited_by_count'),\n",
    "                'author_ids': ', '.join([author['author']['id'] for author in paper.get('authorships', [])]),\n",
    "            }\n",
    "            papers_dataset.append(paper_info)\n",
    "\n",
    "            abstract_info = {\n",
    "                'id': paper.get('id'),\n",
    "                'title': paper.get('title'),\n",
    "                'abstract_inverted_index': paper.get('abstract_inverted_index', '')\n",
    "            }\n",
    "            abstract_dataset.append(abstract_info)\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return papers_dataset, abstract_dataset\n",
    "\n",
    "batches = [list(author_ids.values())[i:i + BATCH_SIZE] for i in range(0, len(list(author_ids.values())), BATCH_SIZE)]\n",
    "\n",
    "# Run in parallel\n",
    "num_cores = 12  # Adjust based on your CPU\n",
    "results = Parallel(n_jobs=num_cores)(\n",
    "    delayed(fetch_papers_for_authors)(batch, idx, len(batches))\n",
    "    for idx, batch in tqdm(enumerate(batches), total=len(batches))\n",
    ")\n",
    "\n",
    "papers_dataset = [paper for res in results for paper in res[0]]\n",
    "abstract_dataset = [abstract for res in results for abstract in res[1]]\n",
    "\n",
    "df_papers = pd.DataFrame(papers_dataset)\n",
    "df_abstracts = pd.DataFrame(abstract_dataset)\n",
    "\n",
    "df_papers.to_csv('papers_dataset.csv', index=False)\n",
    "df_abstracts.to_csv('abstracts_dataset.csv', index=False)\n",
    "\n",
    "print(\"Processing complete. Datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8673d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3797 3797\n"
     ]
    }
   ],
   "source": [
    "print(len(df_papers), len(df_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65f06823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>works_api_url</th>\n",
       "      <th>h_index</th>\n",
       "      <th>works_count</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/A5053043999</td>\n",
       "      <td>Aaron J. Schein</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/A5061449471</td>\n",
       "      <td>Afra Mashhadi</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>16</td>\n",
       "      <td>91</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/A5033831688</td>\n",
       "      <td>Alberto Acerbi</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>29</td>\n",
       "      <td>162</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/A5007176508</td>\n",
       "      <td>Alex Pentland</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>122</td>\n",
       "      <td>1033</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/A5029104172</td>\n",
       "      <td>Alex Rutherford</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>https://openalex.org/A5090107603</td>\n",
       "      <td>Zsófia Rakovics</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>HU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>https://openalex.org/A5100771200</td>\n",
       "      <td>Muhammad Shafiq</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>49</td>\n",
       "      <td>396</td>\n",
       "      <td>PK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>https://openalex.org/A5087528940</td>\n",
       "      <td>Diogo A. Gomes</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>30</td>\n",
       "      <td>253</td>\n",
       "      <td>SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>https://openalex.org/A5054348632</td>\n",
       "      <td>Ákos Huszár</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>HU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>https://openalex.org/A5093136191</td>\n",
       "      <td>Şükrü Atsızelti</td>\n",
       "      <td>https://api.openalex.org/works?filter=author.i...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>TR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id     display_name  \\\n",
       "0    https://openalex.org/A5053043999  Aaron J. Schein   \n",
       "1    https://openalex.org/A5061449471    Afra Mashhadi   \n",
       "2    https://openalex.org/A5033831688   Alberto Acerbi   \n",
       "3    https://openalex.org/A5007176508    Alex Pentland   \n",
       "4    https://openalex.org/A5029104172  Alex Rutherford   \n",
       "..                                ...              ...   \n",
       "707  https://openalex.org/A5090107603  Zsófia Rakovics   \n",
       "708  https://openalex.org/A5100771200  Muhammad Shafiq   \n",
       "709  https://openalex.org/A5087528940   Diogo A. Gomes   \n",
       "710  https://openalex.org/A5054348632      Ákos Huszár   \n",
       "711  https://openalex.org/A5093136191  Şükrü Atsızelti   \n",
       "\n",
       "                                         works_api_url  h_index  works_count  \\\n",
       "0    https://api.openalex.org/works?filter=author.i...       16           19   \n",
       "1    https://api.openalex.org/works?filter=author.i...       16           91   \n",
       "2    https://api.openalex.org/works?filter=author.i...       29          162   \n",
       "3    https://api.openalex.org/works?filter=author.i...      122         1033   \n",
       "4    https://api.openalex.org/works?filter=author.i...       13           53   \n",
       "..                                                 ...      ...          ...   \n",
       "707  https://api.openalex.org/works?filter=author.i...        2            6   \n",
       "708  https://api.openalex.org/works?filter=author.i...       49          396   \n",
       "709  https://api.openalex.org/works?filter=author.i...       30          253   \n",
       "710  https://api.openalex.org/works?filter=author.i...        5           31   \n",
       "711  https://api.openalex.org/works?filter=author.i...        1            7   \n",
       "\n",
       "    country_code  \n",
       "0             US  \n",
       "1             US  \n",
       "2             IT  \n",
       "3             US  \n",
       "4             DE  \n",
       "..           ...  \n",
       "707           HU  \n",
       "708           PK  \n",
       "709           SA  \n",
       "710           HU  \n",
       "711           TR  \n",
       "\n",
       "[712 rows x 6 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd04484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3104"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers['author_ids'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03470d",
   "metadata": {},
   "source": [
    "There are 3797 works listed in my IC2S2 papers dataframe. Here there are 3104 unique co-authors for these works.\n",
    "\n",
    "To improve the efficiency of the code, joblib.parallel was implemented, which gave the biggest boost in efficiency. Searching for multiple authors at the same time also helped a lot, where it was chosen to search for 20 authors at a time, since this would be a very stable yet still effective number. I also added error handling & retries to prevent failures, so that if the api failed, it could just be retried, and the entire program wouldn't have to restart.\n",
    "\n",
    "For the choices of filtering, keeping the works count for an author between 5 and 5000 makes sure that we only have serious authors that both have actually contributed and also don't just put their name on anything. The filter that papers needed at least 10 citations is done to show that the papers have an actual impact, and the co-authors to fewer than 10 filter highlights more focused research. We also filtered for topics in computational social science, like sociology and somputer science, to keep the dataset relevant. However, this can make it so ones with topics in different fields that might still have great impact on computational social science, are filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72dca3-246a-4056-b99c-2f14ccef7fef",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists\n",
    "Week 4, ex 1. Please use the final dataset you collected from both authors and co-authors (IC2S2 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147f812-b857-4b46-a762-fcb8661cb47c",
   "metadata": {},
   "source": [
    "> **Exercise: Constructing the Computational Social Scientists Network**\n",
    ">\n",
    "> In this exercise, we will create a network of researchers in the field of Computational Social Science using the NetworkX library. In our network, nodes represent authors of academic papers, with a direct link from node _A_ to node _B_ indicating a joint paper written by both. The link's weight reflects the number of papers written by both _A_ and _B_.\n",
    ">\n",
    "> **Part 1: Network Construction**\n",
    ">\n",
    "> 1. **Weighted Edgelist Creation:** Start with your dataframe of *papers*. Construct a _weighted edgelist_ where each list element is a tuple containing three elements: the _author ids_ of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once. \n",
    ">\n",
    "> 2. **Graph Construction:**\n",
    ">    - Use NetworkX to create an undirected [``Graph``](https://networkx.org/documentation/stable/reference/classes/graph.html).\n",
    ">    - Employ the [`add_weighted_edges_from`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_weighted_edges_from.html#networkx.Graph.add_weighted_edges_from) function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph.\n",
    ">\n",
    "> 3. **Node Attributes:**\n",
    ">    - For each node, add attributes for the author's _display name_, _country_, _citation count_, and the _year of their first publication_ in Computational Social Science. The _display name_ and _country_ can be retrieved from your _authors_ dataset. The _year of their first publication_ and the _citation count_  can be retrieved from the _papers_ dataset.\n",
    ">    - Save the network as a JSON file.\n",
    ">      \n",
    "> **Part 2: Preliminary Network Analysis**\n",
    "> Now, with the network constructed, perform a basic analysis to explore its features.\n",
    "> 1. **Network Metrics:**\n",
    ">    - What is the total number of nodes (authors) and links (collaborations) in the network? \n",
    ">    - Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.\n",
    ">    - Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
    ">    - If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset. \n",
    ">    - How many isolated nodes are there in your network?  An isolated node is defined as a node with no connections to any other node in the network.\n",
    ">    - Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why?  __(answer in max 150 words)__\n",
    "> \n",
    "> 3. **Degree Analysis:**\n",
    ">    - Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us about the network? __(answer in max 150 words)__\n",
    "> \n",
    "> 4. **Top Authors:**\n",
    ">    - Identify the top 5 authors by degree. What role do these node play in the network? \n",
    ">    - Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? __(answer in max 150 words)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35130cc2-dbf6-42db-8901-2475ede6444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Co-authorship network saved as JSON!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "papers_df = pd.read_csv('papers_dataset.csv', header=0, dtype=str) \n",
    "papers_df.columns = [\"id\",\"publication_year\",\"cited_by_count\",\"author_ids\"]\n",
    "\n",
    "authors_df = pd.read_csv('authors_data_single_name_search.csv')\n",
    "\n",
    "papers_df[\"cited_by_count\"] = pd.to_numeric(papers_df[\"cited_by_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "papers_df[\"publication_year\"] = pd.to_numeric(papers_df[\"publication_year\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "coauthor_counts = {}\n",
    "author_citations = {}\n",
    "author_first_pub_year = {}\n",
    "\n",
    " \n",
    "for _, row in papers_df.iterrows():\n",
    "    if pd.isna(row[\"author_ids\"]):\n",
    "        continue\n",
    "    \n",
    "    author_list = [author.strip() for author in row[\"author_ids\"].split(\",\")]\n",
    "    author_list = list(set(author_list)) \n",
    "\n",
    "    \n",
    "    for author in author_list:\n",
    "        author_citations[author] = author_citations.get(author, 0) + row[\"cited_by_count\"]\n",
    "        \n",
    "        if author not in author_first_pub_year:\n",
    "            author_first_pub_year[author] = row[\"publication_year\"]\n",
    "        else:\n",
    "            author_first_pub_year[author] = min(author_first_pub_year[author], row[\"publication_year\"])\n",
    "\n",
    "    for author1, author2 in combinations(sorted(author_list), 2):\n",
    "        coauthor_counts[(author1, author2)] = coauthor_counts.get((author1, author2), 0) + 1\n",
    "\n",
    "weighted_edgelist = [(author1, author2, weight) for (author1, author2), weight in coauthor_counts.items()]\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_weighted_edges_from(weighted_edgelist)\n",
    "\n",
    "for _, row in authors_df.iterrows():\n",
    "    author_name = row[\"display_name\"].strip()\n",
    "    author_id = row[\"id\"]\n",
    "    \n",
    "    if author_id in G.nodes:\n",
    "        G.nodes[author_id][\"display_name\"] = author_name\n",
    "        G.nodes[author_id][\"country\"] = row.get(\"country_code\", \"Unknown\")\n",
    "        G.nodes[author_id][\"citation_count\"] = author_citations.get(author_id, 0)\n",
    "        G.nodes[author_id][\"first_pub_year\"] = author_first_pub_year.get(author_id, \"Unknown\")\n",
    "\n",
    "graph_data = nx.node_link_data(G)\n",
    "\n",
    "with open(\"coauthorship_network.json\", \"w\") as f:\n",
    "    json.dump(graph_data, f, indent=4)\n",
    "\n",
    "print(\"✅ Co-authorship network saved as JSON!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
